## Output template

**Prompt ID:** (records the prompt this was derived from)  
**LLM:** (records the model and/or variant)  
**Platform:**  (because the same model can be accessed in different ways - e.g. web UI or API - and because that can influence the output, the platform is recorded too). 
**Context:** (because the output is determined by not only the prompt but also the contexual data that is supplied, these are also recorded)   
**Settings:** (because the output is also determined by the settings used - like the temperature - this is recorded also).
**Instructions or agent:** (prompts are sometimes run against standard LLMs and sometimes run against fine-tunes or LLM 'assistants' whose outputs are guided not only by inputs and context, but also by 'instructions' which persist across interactions. These are noted here)